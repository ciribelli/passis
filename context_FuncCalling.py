import numpy as np
import pandas as pd
from openai import OpenAI
from scipy.spatial.distance import cosine
from dotenv import load_dotenv
import os
import json
import main
import app
import send_msg

client = OpenAI()

first_item = ""
def create_context(question, df, max_len=1200, size="ada"):
    load_dotenv()
    client.api_key = os.getenv('OPENAI_API_KEY')
    # Recupera os embeddings para a pergunta em formato dataFrame - fun√ß√£o n√£o documentada pela opeanAI
    q_embeddings = client.embeddings.create(input=question, model='text-embedding-ada-002').data[0].embedding
    df["distances"] = df["embeddings"].apply(lambda x: cosine(q_embeddings, x))

    returns = []
    links = []
    cur_len = 0

    # Ordena por dist√¢ncias e adiciona o texto para ao contexto respeitando o m√°ximo tamanho configurado em max_len
    for i, row in df.sort_values('distances', ascending=True).iterrows():
        # Adiciona o tamanho do texto ao current length do contexto
        cur_len += row['n_tokens'] + 4
        # Interrompe se o contexto j√° √© longo o suficiente
        if cur_len > max_len:
            break
        # Adiciona o texto que ser√° retornado / adiciona tamb√©m informa√ß√µes da tabela e o √≠ndice correspondente para futura recupera√ß√£o
        returns.append(row["texto"])
        links.append(str(row["tabela"])+'/'+str(row["index"]))

    global first_item
    # Guarda a posi√ß√£o do primeiro item de menor dist√¢ncia da pergunta
    first_item = links[0]
    # Retorna o contexto
    return "\nü§ñ\n".join(returns)

def remove_spec_char(sentence):
    cleaned_sentence = sentence.replace('"', ' ').replace("'", ' ')
    return cleaned_sentence

def answer_question(
    df,
    data_atual="alguma data",
    hora_atual="alguma hora",
    phone_number_id = "algum num",
    from_number = "algum codigo",
    question="aqui vem a pergunta",
    lista_threads="aqui estarao as threads",
    max_len=800,
    size="ada",
    debug=True,
    eh_pergunta=False,
):

    context = create_context(question, df, max_len=max_len, size=size,)

    if debug:
        print("Context:\n" + context)
        print("\n\n")

    tools = [
        {
            "type": "function",
            "function": {
                "name": "busca_Jogos",
                "description": "Busca uma lista de jogos de futebol para uma data espec√≠fica.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "date": {
                            "type": "string",
                            "description": "A data dos jogos deve ser informada no formato dd-mm-yyyy",
                        }
                    },
                    "required": ["date"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "obter_cidade_atual_e_clima",
                "description": "Busca informa√ß√µes sobre minha localiza√ß√£o (em qual cidade estou) e o clima desta regi√£o (temperatura, vento, umidade, probabilidade de chuva) num intervalo de tempo compreendido entre uma data espec√≠fica e a data atual.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "date": {
                            "type": "string",
                            "description": "A data de in√≠cio da janela do intervalo deve ser informada no formato dd-mm-yyyy",
                        }
                    },
                    "required": ["date"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "busca_Checkin",
                "description": "Busca uma lista de checkins num intervalo compreendido entre uma data espec√≠fica e a data atual. Checkins podem ser compromissos quaisquer tais como, hora que acorda, hora que bebe √°gua, hora que foi √† academia, hora que chegou ao trabalho ou algum lugar. O atributo direction descreve se algu√©m est√° entrando ou saindo do checkin (in ou out). Por exemplo, checkin 'awake' direction 'out' quer dizer 'dormir'. Checkins N√ÉO trazem informa√ß√£o sobre minha localiza√ß√£o",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "date": {
                            "type": "string",
                            "description": "A data de in√≠cio da janela do intervalo deve ser informada no formato dd-mm-yyyy",
                        }
                    },
                    "required": ["date"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "ultimo_Checkin",
                "description": "fun√ß√£o para buscar detalhes do √∫ltimo checkin realizado pelo usu√°rio no banco de dados. Atributos como tipo de checkin, hora, e direction podem ser o objetivo do usu√°rio. Quando o usu√°rio perguntar 'qual foi o meu √∫ltimo checkin?', ou 'a que horas foi meu ultimo checkin?', essa √© a fun√ß√£o a ser chamada",
            },
        },
        {
            "type": "function",
            "function": {
                "name": "busca_Clima",
                "description": "Busca informa√ß√µes em tempo real para o clima da especificamente para a cidade do Rio de Janeiro. Informa√ß√µes como temperatura, precipita√ß√£o, ventos, dentre outras. N√£o demanda par√¢metros de entrada.",
                "parameters": {},
            },
        },
        {
            "type": "function",
            "function": {
                "name": "busca_Cidade",
                "description": "Busca informa√ß√µes em tempo real sobre o tr√¢nsito e eventos na cidade do Rio de Janeiro. Fonte das informa√ß√µes √© Centro de Opera√ß√µes Rio.",
                "parameters": {},
            },
        },
                {
            "type": "function",
            "function": {
                "name": "musk_knows",
                "description": "Essa fun√ß√£o simplesmente entende quando o usu√°rio quer saber algo sobre Elon Musk. Uma vez que isso seja detectado, a fun√ß√£o busca os √∫ltimos 5 twittes do Elon Musk no X.",
                "parameters": {},
            },
        },
        {
            "type": "function",
            "function": {
                "name": "registra_Memoria",
                "description": "Essa fun√ß√£o simplesmente entende quando o usu√°rio quer salvar uma informa√ß√£o qualquer na mem√≥ria eterna.",
                "parameters": {},
            },
        },
    ]

    messages = [
        {
            "role": "system",
            "content": "Voc√™ √© meu assistente pessoal e seus assuntos de dom√≠nio s√£o minhas mem√≥rias, localiza√ß√£o, compromissos (checkins), clima e o tr√¢nsito." + "Saiba que hoje √© " + data_atual + " e agora s√£o " + hora_atual + "\n" + "Formate todas as respostas para o WhatsApp usando: asteriscos para negrito, underscores para it√°lico, - para bullets, 1. 2. 3. para listas numeradas, e mantenha frases curtas para boa leitura. N√£o use HTML nem Markdown." + "Receba abaixo minhas informa√ß√µes pessoais:" + "\n\n\n << >> \n\n\n" + context + "\n"
        }
    ]

    try:
        for thread in reversed(lista_threads):
            try:
                messages.append(json.loads(thread[0], strict=False))
            except json.JSONDecodeError as e:
                print(f"Erro de decodifica√ß√£o JSON: {e}")

        messages.append({"role": "user", "content": question})
        print("mensagens: \n", messages)

        completion = client.chat.completions.create(
            #model="gpt-3.5-turbo-0125",
            model="gpt-4o",
            messages=messages,
            tools=tools, # para chamada da funcao
            tool_choice="auto", # para chamada da funcao
        )
        
        respostas = completion.choices[0].message.tool_calls

        if respostas:
            # Para cada objeto na lista, extrair as informa√ß√µes relevantes e chamar a fun√ß√£o
            # Envio uma mensagem para reduzir ansiedade
            send_msg.send_wapp_msg(phone_number_id, from_number, "‚è≥ _Avaliando pra voc√™.._")
            for resposta in respostas:
                function_name = resposta.function.name
                function_args = json.loads(resposta.function.arguments)

                # Chamar a fun√ß√£o com base nas informa√ß√µes extra√≠das
                if function_name == 'busca_Clima':
                    token = os.getenv('token_clima')
                    function_output, datajson = main.busca_Clima(token)
                    print("\nSaida para busca_Clima:\n", function_output)
                if function_name == 'busca_Jogos':
                    function_output = main.get_jogos(function_args.get("date"))
                    print("\nSaida para busca_Jogos:\n", function_output, "\nData alvo sugerida pela funcao:\n", function_args.get("date"))
                if function_name == 'busca_Cidade':
                    # 226409689 id Operacoes Rio
                    token = os.getenv('token_X')
                    function_output, datajson = main.busca_X2(token, "226409689")
                    print("\nSaida para busca_Cidade:\n", function_output)
                if function_name == 'musk_knows':
                    # 44196397 id Elon Musk
                    token = os.getenv('token_X')
                    function_output, datajson = main.busca_X2(token, "44196397")
                    print("\nSaida para Musk:\n", function_output)
                if function_name == 'obter_cidade_atual_e_clima':
                    print('estou na fun√ß√£o obter cidade e clima')
                    print('data alvo:')
                    print(function_args.get("date"))
                    text_output, datajson = app.obter_cidade_atual_e_clima(function_args.get("date"), data_atual)
                    df_result_from_json = pd.read_json(datajson, orient='records', convert_dates=['data'])
                    # convertendo json para dataframe e dataframe into text para melhor expericia com LLM
                    function_output = df_result_from_json.to_string(index=False)
                    print("\nSaida para obter_cidade_atual_e_clima:\n", function_output, "\nData alvo sugerida pela funcao:\n", function_args.get("date"))
                if function_name == 'busca_Checkin':
                    text_output, datajson = app.get_checkins_by_date(function_args.get("date"), data_atual)
                    #converter json para dataframe
                    df_result_from_json = pd.read_json(datajson, orient='records', convert_dates=['data'])
                    # convertendo json para dataframe e dataframe into text para melhor expericia com LLM
                    function_output = df_result_from_json.to_string(index=False)
                    print("\nSaida para busca_Checkin:\n", function_output, "\nData alvo sugerida pela funcao:\n", function_args.get("date"))
                if function_name == 'ultimo_Checkin':
                    function_output = app.get_last_checkin_details()
                    print("\nSaida para ultimo_Checkin:\n", function_output)
                if function_name == 'registra_Memoria':
                    eh_pergunta = True
                    function_output = "pergunte ao usu√°rio se ele quer salvar essas informa√ß√µes. Apresente as informa√ß√µes que ele pediu e confirme se ele quer mesmo."
                    print("\nSaida para registra_Memoria, pois isso claramente √© um pedido para salvar na mem√≥ria:\n", function_output)
                messages.append(
                    {
                        "tool_call_id": resposta.id,
                        "role": "assistant",
                        "name": function_name,
                        "content": function_output,
                    })

            client.base_url="https://api.x.ai/v1"
            client.api_key = os.getenv('api_key_grok')
            second_response = client.chat.completions.create(
                model="grok-3-latest",
                messages=messages,
                temperature=0.1  # Valor baixo para respostas mais determin√≠sticas
            )

            # print('\n\n\n **_dentro do if que chama funcao_** \n\n\n')
            # print("mensagens: \n", messages)
            return second_response.choices[0].message.content.strip(), eh_pergunta, messages
        else:
            # print('\n\n\n **_fora do if que chama funcao_** \n\n\n')
            # print("mensagens: \n", messages)
            return completion.choices[0].message.content.strip(), eh_pergunta, messages

    except Exception as e:
        print('Erro no m√©todo completions: ', e)
        send_msg.send_wapp_msg(phone_number_id, from_number, "Aconteceu algo errado ü´§")
        return "", eh_pergunta, messages


def audio_transcription():

    audio_file = open("arquivo.mp3", "rb")

    transcription = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file
    )
    return (transcription.text)
    print(transcription.text)

def responde_emb(pergunta, dados, threads, data_atual, hora_atual, phone_number_id, from_number):
    df = pd.DataFrame(dados)
    df['embeddings'] = df['embeddings'].apply(np.array)
    resposta, tipo, prompt_final = answer_question(df, data_atual, hora_atual, phone_number_id, from_number, question=pergunta, lista_threads=threads, )
    global first_item
    return resposta, first_item, tipo, prompt_final
